I"ç
<h3 id="where-things-stand">Where Things Stand</h3>
<p>Initial experiments are promising, when using the sorted Laplacian spectrum as the features for building a classifier model. Even with small samples, it seems to show the following:</p>
<ul>
<li>We can tell lineage splitting from a complete network from a probabilistic nearest neighbor model</li>
<li>We canâ€™t tell PNN models from each other given different shapes (aspect ratio) to the region</li>
</ul>
<p>This comes from building a multi-class GB tree model from <code>sc-1</code>, <code>sc-3</code>, <code>sc-4-nn</code>, and <code>sc-4-nn</code>, and predicting the data generating model from a 10% holdout set.</p>
<p>The classifier results hold pretty steady in a qualitative sets regardless of the random train/test split.</p>
<p>What doesnâ€™t hold steady is the prediction and class probabilities for the PFG continuity graph. I get different answers depending upon the train/test split, which is probably a function of:</p>
<ul>
<li>Insufficient diversity in the network models used in simulation â€“ I need many examples of each network model</li>
<li>Sample size overall</li>
</ul>
<p>Given that there isnâ€™t much overlap in the overall classification itself, my guess is that if we could look at this in the 10 dimensional space of the eigenvalues used, we would see that:</p>
<ul>
<li>The PFG seriation is actually not deeply embeeded in the convex hull of points for any of the classes, but is near the edge of several or even all</li>
<li>That the decision boundaries shift a lot with respect to the area where the PFG seriation is located</li>
</ul>
<p>Given this, a different train/test split could shift a decision boundary very slightly, without having a major impact on the overall confusion matrix among models, and thus change the predicted assignment for the PFG sample.</p>
<p>We might be able to visualize something like the above by using a dimensionality reduction technique and mapping the models against say the first 3 principal components, and then putting PFG on the map. Worth a try.</p>
<h3 id="computational-next-steps">Computational Next Steps</h3>
<p>But removing this issue and getting stable predictions for PFG is going to be a function of:</p>
<ul>
<li>More classes of network models, since PFG might not really be well described by any of the existing ones</li>
<li>More network models per class</li>
<li>More samples of simulations per network model (or model class)</li>
</ul>
<p>While I develop more network models, I will probably start doing the second and third for the existing four models, but with PNN models collapsed down to a single model. I donâ€™t have the formal infrastructure yet for doing multiple realizations of a single network model, so thatâ€™s the first step.</p>
:EF