I"+B<h3 id="background">Background</h3>
<p>Traditional archaeological seriation generated solutions that always had a static linear order, either by use of a multidimensional scaling algorithm or via Fordian manual shuffling of assemblages. In such context, departures from a “correct” solution can be measured by the number of “out of order” assemblages (or via a more opaque “stress” statistic in some cases).</p>
<p>Last year Carl Lipo and I <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0124942">introduced a method</a> for finding sets of classical Fordian seriations from a set of assemblages via an agglomerative iterative procedure <span class="citation" data-cites="Lipo2015">(Lipo 2015)</span>. We called the resulting seriations “iterative deterministic seriation solutions” or IDSS. IDSS is capable of using any metric for joining assemblages, including classic unimodality, or various distance measures (something we’ll be <a href="http://notebook.madsenlab.org/essays/2015/09/06/saa2016-abstract.html">talking about at the SAA meetings this year</a>).</p>
<p>When IDSS finds groups of data points that seriate together, but do not seriate (within thresholds) with other groups of points, several solutions are created rather than forcing all of the data points into a single scaling or solution (as has been typical practice with statistical seriation by archaeologists in the past). Instead, IDSS outputs all valid solution sets, since the discontinuities between solution sets may indicate lack of cultural interaction, poor recovery sampling of the archaeological record, or other real-world factors.</p>
<p>Frequently, one assemblage or data point will occur in multiple solutions, and when this occurs IDSS overlays the solutions and forms a tree. From our paper, Figure 1 shows an IDSS frequency seriation solution for archaeological sites in the Central Mississippi River Valley. Each assemblage is represented by decorated ceramic class frequencies, and we can see that there are at least three major solutions which share assemblage 13-O-7, and a number of minor branches.</p>
<figure>
<img src="/images/idss-fig12-pfg-solution.png" alt="Figure 1: IDSS seriation solution for PFG sites in Cental Mississippi River Valley" /><figcaption>Figure 1: IDSS seriation solution for PFG sites in Cental Mississippi River Valley</figcaption>
</figure>
<h3 id="the-problem">The Problem</h3>
<p>My current research is aimed at developing ways of treating IDSS seriation solution graphs as data points in machine learning algorithms, with the goal of fitting coarse grained, regional models of cultural transmission and information diffusion to our data. Seriations are the perfect tool for capturing regional-scale social influences that are time-transgressive.</p>
<p>There are several challenges in this work. The first, which is described in a <a href="http://localhost:4000/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriationct/2014/06/17/seriationct-requirements.html">previous post</a> is to create regional metapopulation models with cultural transmission that yield seriations, rather than just type or variant frequencies, as their observable output.</p>
<p>The second challenge is then to find an inferential framework for model selection which allows one to measure the goodness-of-fit between the theoretical model’s output, and specific empirical seriations like Figure 1.</p>
<p>This second challenge is perfectly suited to an Approximate Bayesian Computation approach <span class="citation" data-cites="Beaumont2002 Csillery2010 Marin2012 Toni2009">(Beaumont, Zhang, and Balding 2002; Csilléry et al. 2010; Marin et al. 2012; Toni et al. 2009)</span>, since while we can simulate data from each social learning model, writing down the likelihood function for each model is generally an intractable problem.</p>
<p>In brief, ABC model selection involves simulating a large number of synthetic data points from each model, calculating summary statistics from those data points, measuring the distance (losses) between summary statistics and observed data, and choosing the model whose losses are the smallest.</p>
<p>In my current work, the “models” are actually a combination of:</p>
<ol type="1">
<li>A social learning model (e.g., unbiased cultural transmission or conformist transmission)</li>
<li>Parameters for that social learning model (e.g., innovation rates)</li>
<li>A spatiotemporal population model that describes how communities evolved in a region and what the pattern of community interaction was over time</li>
</ol>
<p>This is a fairly complex “model” to fit, but we can easily simulate samples of cultural variant frequencies across subpopulations from each model (using the <a href="https://github.com/mmadsen/seriationct"><code>SeriationCT</code> software suite</a>, and then use IDSS to seriate those variant frequencies, yielding simulated versions of Figure 1.</p>
<p>What we need, then, is a way to measure the distance or loss between Figure 1 and various simulated seriation graphs. Once we have a good loss function, we should be able to perform ABC model selection on seriation graphs.</p>
<h3 id="distanceloss-for-unlabelled-unordered-graphs">Distance/Loss for Unlabelled, Unordered Graphs</h3>
<p>In general, we want a function which measures the structural similarity of two graphs. There are many approaches to the problem (see <span class="citation" data-cites="zager2008graph">(Zager and Verghese 2008)</span> for a review). Graph edit distance would be a fairly natural metric, but most algorithms for edit distance rely on matching node identities, and many algorithms also rely upon graphs being ordered as well as labelled.</p>
<p>The simulated seriations which come out of the <code>SeriationCT</code> package are unlabelled, and we can’t label them with the identities of the assemblages in our empirical data. So our loss function has to measure structural similarity without reference to node identity, or any notion of ordering or orientation for the graphs.</p>
<p>This leads fairly directly to using purely algebraic properties of the seriation graphs, and in particular the spectral properties of various matrices associated with the graphs. We know that the eigenvalues of the (possibly binarized) adjacency matrix are related to the edge structure of a graph, and that the eigenvalues of the Laplacian of the adjacency matrix are an even more sensitive indicator of structure <span class="citation" data-cites="godsil2001algebraic">(Godsil and Royle 2001)</span>.</p>
<p>In a technical report, Koutra et al. <span class="citation" data-cites="koutra2011algorithms">(Koutra et al. 2011)</span> suggest using the sum of squared differences between the Laplacian spectra of two graphs, trimming the spectra to use only 90% of the total eigenvalue weight. This is attractive from a statistical perspective, essentially giving us the L2 loss between Laplacian spectra for two graphs.</p>
<p>Given adjacency matrices <span class="math inline">\(A_1\)</span> and <span class="math inline">\(A_2\)</span> for graphs <span class="math inline">\(G_1\)</span> and <span class="math inline">\(G_2\)</span>, the Laplacian matrix is simply <span class="math inline">\(L_1 = D_1 - A_1\)</span> and <span class="math inline">\(L_2 = D_2 - A_2\)</span>, where <span class="math inline">\(D_i\)</span> are the corresponding diagonal matrix of vertex degrees. The spectrum is then the set of eigenvalues <span class="math inline">\((\lambda_1 \ldots \lambda_n)\)</span>. Since the Laplacian is positive semi-definite, all of the eigenvalues in the spectrum are positive real numbers. It is possible to put some bounds on the eigenvalues, and thus predetermine the range of possible loss function values for graphs with a given number of vertices, but I leave that to a future post.</p>
<p>If we use the full spectrum of values, our L2 spectral loss function is then:</p>
<p><span class="math inline">\(\mathcal{L}_{sp} = \sum_{i=1}^{n} (\lambda_{1i} - \lambda_{2i})^2\)</span></p>
<p>and if we only use a trimmed set of eigenvalues (say, those which contribute to the top 90% of the total sum of the spectrum), we sort the list of those <span class="math inline">\(k\)</span> eigenvalues and replace <span class="math inline">\(n\)</span> with <span class="math inline">\(k\)</span> in the previous equation.</p>
<h3 id="implementation">Implementation</h3>
<p>In Python, we can implement this loss function for NetworkX graph objects as follows (now part of the <code>seriationct.analytics</code> module):</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" title="1"></a>
<a class="sourceLine" id="cb1-2" title="2"><span class="im">import</span> networkx <span class="im">as</span> nx</a>
<a class="sourceLine" id="cb1-3" title="3"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb1-4" title="4"></a>
<a class="sourceLine" id="cb1-5" title="5"><span class="kw">def</span> graph_spectral_similarity(g1, g2, threshold <span class="op">=</span> <span class="fl">0.9</span>):</a>
<a class="sourceLine" id="cb1-6" title="6">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb1-7" title="7"><span class="co">    Returns the eigenvector similarity, between [0, 1], for two NetworkX graph objects, as</span></a>
<a class="sourceLine" id="cb1-8" title="8"><span class="co">    the sum of squared differences between the sets of Laplacian matrix eigenvalues that account</span></a>
<a class="sourceLine" id="cb1-9" title="9"><span class="co">    for a given fraction of the total sum of the eigenvalues (default = 90%).</span></a>
<a class="sourceLine" id="cb1-10" title="10"></a>
<a class="sourceLine" id="cb1-11" title="11"><span class="co">    Similarity scores of 0.0 indicate identical graphs (given the adjacency matrix, not necessarily</span></a>
<a class="sourceLine" id="cb1-12" title="12"><span class="co">    node identity or annotations), and large scores indicate strong dissimilarity.  The statistic is</span></a>
<a class="sourceLine" id="cb1-13" title="13"><span class="co">    unbounded above.</span></a>
<a class="sourceLine" id="cb1-14" title="14"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb1-15" title="15">    l1 <span class="op">=</span> nx.spectrum.laplacian_spectrum(g1, weight<span class="op">=</span><span class="va">None</span>)</a>
<a class="sourceLine" id="cb1-16" title="16">    l2 <span class="op">=</span> nx.spectrum.laplacian_spectrum(g2, weight<span class="op">=</span><span class="va">None</span>)</a>
<a class="sourceLine" id="cb1-17" title="17">    k1 <span class="op">=</span> _get_num_eigenvalues_sum_to_threshold(l1, threshold<span class="op">=</span>threshold)</a>
<a class="sourceLine" id="cb1-18" title="18">    k2 <span class="op">=</span> _get_num_eigenvalues_sum_to_threshold(l2, threshold<span class="op">=</span>threshold)</a>
<a class="sourceLine" id="cb1-19" title="19">    k <span class="op">=</span> <span class="bu">min</span>(k1,k2)</a>
<a class="sourceLine" id="cb1-20" title="20">    sim <span class="op">=</span> <span class="bu">sum</span>((l1[:k] <span class="op">-</span> l2[:k]) <span class="op">**</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1-21" title="21">    <span class="cf">return</span> sim</a>
<a class="sourceLine" id="cb1-22" title="22"></a>
<a class="sourceLine" id="cb1-23" title="23"></a>
<a class="sourceLine" id="cb1-24" title="24"><span class="kw">def</span> _get_num_eigenvalues_sum_to_threshold(spectrum, threshold <span class="op">=</span> <span class="fl">0.9</span>):</a>
<a class="sourceLine" id="cb1-25" title="25">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb1-26" title="26"><span class="co">    Given a spectrum of eigenvalues, find the smallest number of eigenvalues (k)</span></a>
<a class="sourceLine" id="cb1-27" title="27"><span class="co">    such that the sum of the k largest eigenvalues of the spectrum</span></a>
<a class="sourceLine" id="cb1-28" title="28"><span class="co">    constitutes at least a fraction (threshold, default = 0.9) of the sum of all the eigenvalues.</span></a>
<a class="sourceLine" id="cb1-29" title="29"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb1-30" title="30">    <span class="cf">if</span> threshold <span class="kw">is</span> <span class="va">None</span>:</a>
<a class="sourceLine" id="cb1-31" title="31">        <span class="cf">return</span> <span class="bu">len</span>(spectrum)</a>
<a class="sourceLine" id="cb1-32" title="32"></a>
<a class="sourceLine" id="cb1-33" title="33">    total <span class="op">=</span> <span class="bu">sum</span>(spectrum)</a>
<a class="sourceLine" id="cb1-34" title="34">    <span class="cf">if</span> total <span class="op">==</span> <span class="fl">0.0</span>:</a>
<a class="sourceLine" id="cb1-35" title="35">        <span class="cf">return</span> <span class="bu">len</span>(spectrum)</a>
<a class="sourceLine" id="cb1-36" title="36"></a>
<a class="sourceLine" id="cb1-37" title="37">    spectrum <span class="op">=</span> <span class="bu">sorted</span>(spectrum, reverse<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb1-38" title="38">    running_total <span class="op">=</span> <span class="fl">0.0</span></a>
<a class="sourceLine" id="cb1-39" title="39"></a>
<a class="sourceLine" id="cb1-40" title="40">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(spectrum)):</a>
<a class="sourceLine" id="cb1-41" title="41">        running_total <span class="op">+=</span> spectrum[i]</a>
<a class="sourceLine" id="cb1-42" title="42">        <span class="cf">if</span> running_total <span class="op">/</span> total <span class="op">&gt;=</span> threshold:</a>
<a class="sourceLine" id="cb1-43" title="43">            <span class="cf">return</span> i <span class="op">+</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb1-44" title="44">    <span class="co"># guard</span></a>
<a class="sourceLine" id="cb1-45" title="45">    <span class="cf">return</span> <span class="bu">len</span>(spectrum)</a>
<a class="sourceLine" id="cb1-46" title="46"></a>
<a class="sourceLine" id="cb1-47" title="47"></a>
<a class="sourceLine" id="cb1-48" title="48"></a></code></pre></div>
<h3 id="references-cited" class="unnumbered">References Cited</h3>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-Beaumont2002">
<p>Beaumont, Mark A, W Zhang, and D J Balding. 2002. “Approximate Bayesian computation in population genetics.” <em>Genetics</em>. <a href="http://www.genetics.org/content/162/4/2025.short">http://www.genetics.org/content/162/4/2025.short</a>.</p>
</div>
<div id="ref-Csillery2010">
<p>Csilléry, Katalin, Michael G B Blum, Oscar E Gaggiotti, and Olivier François. 2010. “Approximate Bayesian Computation (ABC) in practice.” <em>Trends in Ecology &amp; Evolution</em> 25 (7): 410–18. <a href="https://doi.org/10.1016/j.tree.2010.04.001">https://doi.org/10.1016/j.tree.2010.04.001</a>.</p>
</div>
<div id="ref-godsil2001algebraic">
<p>Godsil, Christopher David, and Gordon Royle. 2001. <em>Algebraic Graph Theory</em>. Vol. 8. Springer New York.</p>
</div>
<div id="ref-koutra2011algorithms">
<p>Koutra, Danai, Ankur Parikh, Aaditya Ramdas, and Jing Xiang. 2011. “Algorithms for Graph Similarity and Subgraph Matching.” Technical Report of Carnegie-Mellon-University.</p>
</div>
<div id="ref-Lipo2015">
<p>Lipo, Mark E. AND Dunnell, Carl P. AND Madsen. 2015. “A Theoretically-Sufficient and Computationally-Practical Technique for Deterministic Frequency Seriation.” <em>PLoS ONE</em> 10 (4). Public Library of Science: e0124942. <a href="https://doi.org/10.1371/journal.pone.0124942">https://doi.org/10.1371/journal.pone.0124942</a>.</p>
</div>
<div id="ref-Marin2012">
<p>Marin, Jean-Michel, Pierre Pudlo, Christian P Robert, and Robin J Ryder. 2012. “Approximate Bayesian Computational Methods.” <em>Statistics and Computing</em> 22 (6). Springer: 1167–80.</p>
</div>
<div id="ref-Toni2009">
<p>Toni, Tina, David Welch, Natalja Strelkowa, Andreas Ipsen, and Michael P H Stumpf. 2009. “Approximate Bayesian computation scheme for parameter inference and model selection in dynamical systems.” <em>Journal of Royal Society Interface</em> 6 (31). The Royal Society: 187–202. <a href="https://doi.org/10.1098/rsif.2008.0172">https://doi.org/10.1098/rsif.2008.0172</a>.</p>
</div>
<div id="ref-zager2008graph">
<p>Zager, Laura A, and George C Verghese. 2008. “Graph Similarity Scoring and Matching.” <em>Applied Mathematics Letters</em> 21 (1). Elsevier: 86–94.</p>
</div>
</div>
:ET