I"±-<p>Chris Stucchio‚Äôs recent post <a href="http://www.chrisstucchio.com/blog/2013/hadoop_hatred.html">‚ÄúDon‚Äôt use Hadoop - your data isn‚Äôt that big‚Äù</a> is a valuable read for many of us in the sciences, whose data are ‚Äúlarge,‚Äù but not ‚Äúbig data‚Äù in the current industry buzzword sense of the term. In the sciences, astronomers doing automated sky sweeps have big data. The physicists are CERN have big data. Mostly, the rest of us have small to medium data. I‚Äôd like to make a case that there are methods and best practices for the analysis of <strong>medium</strong> scale data that have been largely ignored until now. And that folks doing computational studies in any discipline need to think about how to set up an analysis of medium-scale data such that it‚Äôs replicable and accurate.</p>
<h3 id="defining-medium-data">Defining Medium Data</h3>
<p>Small data, of course, are any data set where it doesn‚Äôt matter what tools you use ‚Äì any operation you choose finishes about the time your finger comes off the ‚ÄúEnter‚Äù key. Or within a couple of seconds, tops. Small data are those that fit in an Excel worksheet (although since V2010 or so, a sheet can have 1.048 million rows instead of the old limits of 65K or so, so arguably you can trend into ‚Äúmedium‚Äù data with Excel now as well).<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>Medium data are those where you can‚Äôt easily perform routine operations:</p>
<ul>
<li>Hand edit the data to clean them up, even in a good programmer‚Äôs text editor</li>
<li>Commit the raw data files to Github along with your analysis code because the files balloon your repository and are difficult for people to download</li>
<li>Do simple manipulations like sorting, merging, adding or deleting columns in a GUI tool (e.g., Excel)</li>
</ul>
<p>Medium data are also:</p>
<ul>
<li>Small enough that scripts or programs can do meaningful processing of the data in a reasonsable time frame (minutes or occasionally hours)</li>
<li>Small enough that R or pandas/numpy in Python can load your data into memory<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></li>
</ul>
<p>If your data are larger than this, you do need a parallelized or cluster solution. Perhaps that solution is Hadoop and MapReduce, perhaps it‚Äôs not. Not every problem is easily expressed in the MapReduce paradigm.</p>
<h3 id="a-medium-data-example">A Medium Data Example</h3>
<p>I‚Äôm working with a dataset right now, from my experiments on <a href="http://localhost:4000/tag/classification.html">observing cultural transmission through classifications</a>, that exemplify ‚Äúmedium data.‚Äù The primary data in MongoDB are 70GB, and when exported to 2 CSV files for further processing take up 2GB and 16MB, respectively. (MongoDB has been a terrific choice for primary data storage in my simulations, but hoo boy, it ain‚Äôt space efficient‚Ä¶).</p>
<p>The data set I‚Äôm describing here has 9.504 million rows, with 24 columns aross the two files. The ‚Äúbig‚Äù file has many replicates at the same ‚Äúparameters,‚Äù and requires aggregation (by calculating mean and standard deviation). The small file is a set of measurements which apply to specific parameter combinations in the big file, and thus can be combined with an ‚Äúinner join.‚Äù The end product of pre-processing and data cleaning, therefore, is a single file, with the 9.504 million rows aggregated to form 95,040 rows, with duplicate columns removed after joining.</p>
<p>There are really two approaches to preprocessing these data:</p>
<ol type="1">
<li>Load the files into R data frames, and use <code>plyr</code> and associated packages to join and aggregate the data.<br />
</li>
<li>Write a script or scripts to perform the join, aggregation, and de-duplication.</li>
</ol>
<p>At this scale, both choices are workable. I‚Äôve done it both ways. But there are circumstances where the second approach is superior for this sort of cleaning/preprocessing (you‚Äôre still going to do your actual analysis in R, or at least a combination of R and Python):</p>
<ul>
<li>You will repeat this analysis across many data sets</li>
<li>Your data size will grow over time</li>
<li>The analysis will be part of a real-time interface for users</li>
</ul>
<p>None of these circumstances <em>necessarily</em> rules out the use of R. You can parallelize many things in R, and in this case by combining <code>plyr</code> with <code>doMC</code> (or Hadley‚Äôs upcoming directly parallelized version of <code>plyr</code>). And you can easily use R from the command line or within a long-running server process. But you won‚Äôt be doing it interactively ‚Äì you‚Äôll be implementing the second approach using R as your scripting language (likely using <code>Rscript</code>).</p>
<p>The early stages of my post-processing pipeline are written in Python, while the second half are written in Apache Pig. In this particular case, the first steps in processing involve reading raw simulation output from the database, and then performing transformations (such as identifying trait values to corresponding coarse-grained classification cells). The second half of the post-processing is essentially all record processing ‚Äì joining independent data blobs to each other, removing duplicate columns in the result, and then aggregating rows. This is all standard ‚ÄúSQL-like stuff‚Äù and Pig is perfect for such operations (while setting us up to scale from small to medium to big data sets in the process).</p>
<p><a href="http://pig.apache.org/">Apache Pig</a> is a data analysis platform which sits on top of the Hadoop MapReduce platform, while being capable of running on a single node. This gives your Pig scripts huge potential scalability, coupled with a dead-simple language and operations for processing data.</p>
<h3 id="best-practices">Best Practices</h3>
<p>In working with ‚Äúmedium‚Äù data, I‚Äôve started developing some habits. Perhaps these habits are not full ‚Äúbest practices‚Äù yet, but I‚Äôll share them anyway.</p>
<h4 id="build-and-test-in-small-pieces">Build and Test in Small Pieces</h4>
<p>At the moment, I have 8 separate small scripts which take original simulation run data in half a dozen database ‚Äútables‚Äù (or ‚Äúcollections‚Äù in MongoDB terms), and lead to a final unified CSV file capable of import to R for analysis. Each script does <em>one</em> thing, and saves its output to an output file (or partfiles, in the case of Pig).</p>
<p>This is the scripting equivalent of ‚Äúseparation of concerns‚Äù in programming, and it allows us to verify the step by step correctness of our processing steps, and isolate the problem when something goes wrong. It also allows us to easily construct simple tests for each stage of the process, to verify that correctness.</p>
<h4 id="what-i-dont-need-tests-for-data-cleaning">What? I Don‚Äôt Need Tests for Data Cleaning</h4>
<p>By now, you‚Äôre probably convinced of the need for unit and functional tests for any complex piece of code, especially if you expect other people to use that code, or trust its output. If you‚Äôre not convinced of this fact, I recommend Kent Beck‚Äôs <a href="http://www.xprogramming.com/testfram.htm">original paper on the subject of test frameworks</a>.</p>
<p>But very few people seem to test the little operations we use for ‚Äúmunging‚Äù data into shape for analysis. We can see, right on the screen, that our data look as we expect them to! That was possibly true when your data set fit into a single Excel worksheet, and you could visually verify that it had the right format for columns, the correct number of rows, and so on. When you joined two data files together, it was usually possible to see whether they ‚Äúaligned‚Äù correctly, by eye.</p>
<p>Can you do that by inspection with 1GB of data? 10GB? I can‚Äôt, and neither can you. Instead, you need tests. Tests can be simple ‚Äì one test I always make between steps is <strong>verifying the expected number of rows of output</strong>. If you are processing 1M rows (and 6 columns) through a script which does a calculation and adds the result as a column, you should get 1M rows x 7 columns in your output file.</p>
<p>This is <strong>particularly</strong> important for processing that performs joins or aggregation. Let‚Äôs say you‚Äôre aggregating replicate measurements together. For each column that had replicated measurements, you know how many replicates. Multiply these together to find the total ‚Äúreplication factor.‚Äù If you divide the total number of rows by this replication factor, this is the number of rows of aggregated output that should be produced. Knowing this, you can check whether your join script is doing the correct operations.</p>
<p>Other simple tests include bounds checking on derived values. In one processing step, I reduce a large number of individual samples to various diversity indices. Several of the indices are constrained to lie between 0.0 and 1.0. This simple sanity check led to finding at least one bug in my database queries, incidentally.</p>
<h4 id="script-the-scripts">Script the Scripts</h4>
<p>Breaking your processing chain into small scripts, each performing a task and capable of verification, raises the danger that you‚Äôll execute them in the wrong order. This may or may not lead to overt errors, but if not, a variety of subtle errors can creep into your data. Thus, I always ‚Äúscript the scripts‚Äù ‚Äì keep a simple shell script which executes each processing step in order.</p>
<p>While I‚Äôm writing the scripts and testing, I usually execute each one manually (often many times). But when I actually produce the data set for analysis, I <em>always</em> use the overall ‚Äúdriver‚Äù script to ensure that all of the processing steps happen in the right order.</p>
<h3 id="references-cited" class="unnumbered">References Cited</h3>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-mccullough2008accuracy">
<p>McCullough, BD, and David A Heiser. 2008. ‚ÄúOn the Accuracy of Statistical Procedures in Microsoft Excel 2007.‚Äù <em>Computational Statistics &amp; Data Analysis</em> 52 (10). Elsevier: 4570‚Äì8.</p>
</div>
</div>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>This is not an endorsement of Excel for data analysis. In fact, you <strong>SHOULD NOT</strong> use Excel for data analysis, at least for anything more complex than some sums and averages. The statistics literature is rife with examples of Excel‚Äôs terrible statistics function implementations <span class="citation" data-cites="mccullough2008accuracy">(McCullough and Heiser 2008)</span><a href="http://www.pages.drexel.edu/~bdm25/excel2007.pdf">(Download the [PDF] here)</a>. McCullough has documented Excel‚Äôs statistical failures through successive releases, and notes that flaws publicized over 15 years ago still persist (at least in Excel 2007). Nor are these minor round-off errors ‚Äì examples abound of regression analyses that return the wrong sign for a regression coefficient or correlation coefficient, and so on. If you care about your results, <strong>DO NOT</strong> use Microsoft Excel.<a href="#fnref1" class="footnote-back" role="doc-backlink">‚Ü©</a></p></li>
<li id="fn2" role="doc-endnote"><p>This assumes that you have maxed out your laptop‚Äôs RAM (Mine have 16GB, for precisely this reason) or have upgraded your desktop. If you have large data sets, and are operating on 2 or 4GB of RAM, you should seriously consider upgrading.<a href="#fnref2" class="footnote-back" role="doc-backlink">‚Ü©</a></p></li>
</ol>
</section>
:EF