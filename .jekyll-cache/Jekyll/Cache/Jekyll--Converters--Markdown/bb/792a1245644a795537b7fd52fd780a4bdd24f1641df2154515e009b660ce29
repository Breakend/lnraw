I"è(<h3 id="overview">Overview</h3>
<p>In a previous experiment (<code>axelrod-ct</code>), I managed the EC2 simulation environment manually, with shell scripts to start simulations and to check on the status of runs. It worked, but it‚Äôs a pain in the butt, and it‚Äôs hard to do replicably. <a href="http://star.mit.edu/cluster/index.html">StarCluster</a> is a better solution, but I didn‚Äôt take the time before SAA2014 to become versed in its use.</p>
<p>I am doing so for <code>ctmixtures</code> and <code>seriationct</code>, and thus far I‚Äôm impressed. Amazed, actually. Starcluster abstracts away the process of directly dealing with EC2 instances, and instead creates a single logical cluster of nodes that you can drive from a ‚Äúmaster‚Äù node using simple commands. Some skill at using a Unix environment is pretty essential, although for very simple jobs with an executable and simple output, it might not be necessary.</p>
<p>This note records the steps I took to get my computing cluster configuration working, and small tweaks I‚Äôve made to the main simulation code and ‚Äúrun builder‚Äù scripts to ensure that everything works when you push it to the cluster environment. I spent some time using a 1 node default configuration to understand job processing, the layout and basic commands, and I recommend you do the same. It cost about five bucks of EC2 runtime to get to the point where I could run a test experiment on a true parallel cluster, and was money well spent.</p>
<p>Throughout, I‚Äôll try to highlight things to watch out for or issues/bugs I found, in <strong>bold</strong> type.</p>
<h3 id="installation">Installation</h3>
<p>First, install StarCluster. A slightly outdated version came with Anaconda python, so it‚Äôs worth updating. I had to install it from source to prevent some version conflicts, but you may be able to do:</p>
<pre class="shell"><code>$ pip install --upgrade starcluster</code></pre>
<p>If you encounter any errors whatsoever, download the source from MIT and just install it like any normal Python package.</p>
<h3 id="image-configuration">Image Configuration</h3>
<p>The first task is to configure an AMI or machine image for your cluster nodes. StarCluster provides a number of AMIs which are preconfigured with all of the StarCluster runtimes, Sun Open Grid Engine, and other components which StarCluster relies upon. It is strongly recommended that you start with one of these AMIs.</p>
<p><strong>In fact, of the currently supported StarCluster AMI‚Äôs, you should start with the Ubuntu 12.04 64 bit image, because EC2 has disabled APT repositories for ubuntu versions which are end-of-life. You will not be able to install most software on the 13.04 or recent AMI‚Äôs. StarCluster is aware of this and planning another AMI set based on the next Ubuntu LTS release. By the time you read this, there may be a 14.04 AMI to use</strong></p>
<ol type="1">
<li>Follow the StarCluster user‚Äôs manual and edit <code>~/.starcluster/config</code> to contain paths to apporpriate EC2 RSA and SSH keys. You can use an IAM account for this, which is now recommended rather than your base AWS administration account. You don‚Äôt need to edit much else here at the moment.<br />
</li>
<li>Set up a single cluster host, which we‚Äôll then configure and turn into a custom AMI for all nodes in our cluster. You can use a small instance type here because we‚Äôre not processing, just installing and configuring.</li>
</ol>
<pre class="shell"><code>$ starcluster start -o -s 1 -I m1.small -m ami-765b3e1f imagehost</code></pre>
<p>After a couple of minutes, you can ssh into the master (and only) node for cluster <code>imagehost</code>:</p>
<pre class="shell"><code>$ starcluster sshmaster imagehost</code></pre>
<p>At this point, you can use <code>apt-get</code> to install software, after running <code>apt-get update</code> to refresh the software database, which will be stale given the frozen version in the AMI image. For <code>ctmixtures</code>, I installed:</p>
<ol start="3" type="1">
<li><p><code>mongodb</code> ‚Äì which installs both the server and command line client</p></li>
<li><p><code>build-essential</code> ‚Äì already installed, but I checked for updates</p></li>
<li><p><code>git</code> ‚Äì already installed, but checked for updates</p></li>
<li><p>For Python, I prefer geneally to leave the system Python alone, and use <code>Anaconda Scientific Python</code>:</p></li>
</ol>
<pre class="shell"><code>$ curl -o anaconda2-installer.sh http://09c8d0b2229f813c1b93-c95ac804525aac4b6dba79b00b39d1d3.r79.cf1.rackcdn.com/Anaconda-2.0.1-Linux-x86_64.sh
$ bash anaconda2-installer.sh</code></pre>
<ol start="7" type="1">
<li>Follow the prompts in the text-based installer, accepting the license agreement, and then put Anaconda somewhere accessible, like <code>/usr/local/anaconda</code>. I also allowed the installer to prepend the Anaconda <code>bin</code> directory to root‚Äôs path. <strong>I also should ensure that this is prepended to user <code>sgeadmin</code>‚Äôs path since you execute jobs as this user</strong></li>
<li>I then created an EBS volume of sufficient size (200GB) to hold the MongoDB database on each simulation node, since they can grow quite large during a long simulation run. I did this in the AWS web console, and attached it to <code>/dev/xvdf</code> (the default choice) to my running <code>imagehost-master</code> instance.<br />
</li>
<li>Back on the command line in the <code>imagehost-master</code> itself, I then formatted the new volume, created a mount point, added it to <code>/etc/fstab</code> and mounted it before creating two working directories:</li>
</ol>
<pre class="shell"><code>$ /sbin/mkfs.ext4 /dev/xvdf
$ mkdir /sim
$ ...edited /etc/fstab...
$ mount -a
$ mkdir /sim/src
$ mkdir /sim/data</code></pre>
<ol start="10" type="1">
<li>I then stopped MongoDB, and edited <code>/etc/mongodb.conf</code> to put its <code>datadir</code> in <code>simdata</code> before restarting, and verifying that I could connect to the database via the command line client.<br />
</li>
<li>At this point, inside <code>/sim/src</code>, I cloned the <code>ctmixtures</code> repository at Version 2.1 of the release software, and verified that I could run the various scripts and a simulation on the command line, and that data was inserted into the database at the conclusion of the simulation. This involved installing necessary python dependencies and compiling my module <code>slatkin-exact-tools</code>, which are taken care of by the following:</li>
</ol>
<pre class="shell"><code>$ apt-get install swig
$ pip install -r requirements.txt
$ sh install-slatkin-tools.sh</code></pre>
<ol start="12" type="1">
<li>At this point, the system is capable of running my simulations and gives correct output to a test simulation:</li>
</ol>
<pre class="shell"><code>$ sim-ctmixture-timeaveraging.py --experiment test --debug 1 --configuration conf/allneutral-wellmixed.json --maxinittraits 10 --numloci 4 --conformismstrength 0.3 --anticonformismstrength 0.3 --innovationrate 0.25 --periodic 0 --kandlerinterval 100 --simulationendtime 1000000 --popsize 100 --seed 4207479710594528312</code></pre>
<ol start="12" type="1">
<li><p>Time to turn this into an AMI which is ready to be cloned and run by StarCluster. Exiting the SSH session, back on my local machine, I did the following:</p></li>
<li><p>In the AWS web console, I determined the instance ID of the running <code>imagehost-master</code> instance. Use the <code>ebsimage</code> command to turn this into an EBS AMI, which will freeze a snapshot both of the root and large database partition (your exact instance ID will vary, of course):</p></li>
</ol>
<pre class="shell"><code>$ starcluster ebsimage i-42b409a9 ctmixtures-cluster</code></pre>
<ol start="14" type="1">
<li>After a couple of minutes, a new AMI named <code>ctmixtures-cluster</code> appeared in my AMI list as a private AMI. I then changed its permissions to be public, so anyone can use it to replicate one of my <code>ctmixtures</code> simulations without going through the steps above. The AMI is identified as <code>ami-f668c49e</code>.</li>
</ol>
<h3 id="cluster-configuration-and-operations">Cluster Configuration and Operations</h3>
<p>The simplest thing for configuration is to edit <code>~/.starcluster/config</code> and give <code>ami-f668c49e</code> as the default AMI to use for the default cluster definition. This will allow you to use StarCluster commands without additional arguments, and it will just use your default EC2 keys and this AMI unless otherwise specified. I left the default number of nodes at 4, which is useful for testing, and can be overridden for production work on the command line.</p>
<p>You can start a cluster as follows, assuming the default cluster definition. Choose a name for the cluster (e.g., <code>clustertest</code>):</p>
<pre class="shell"><code>$ starcluster start clustertest</code></pre>
<p>By default, this starts 4 instances using the <code>ctmixtures-cluster</code> AMI, and after a fair bit of automatic configuration, reports that all are ready and SSH is available. You can then use <code>starcluster sshmaster clustertest</code> to log into the master and start jobs, etc.</p>
<p>Stopping the cluster shuts down the instances, but leaves them in the instance list with volumes attached, and they can be restarted if desired. This is useful for stopping the drain on your wallet while you do some analysis and then restart some work.</p>
<p>Terminating the cluster removes the instances from your instance list, but in the current configuration it <strong>DOES NOT remove the 200 GB EBS volumes you created</strong>. Do this manually after you terminate the instances if you do not want to be charged for the EBS storage. I‚Äôm sure there‚Äôs an instance and volume setting to have the volumes auto-delete, but I haven‚Äôt set that up yet.</p>
<p>When you have NO running instances, you should have no volumes associated with this cluster and image, but there should be a 10GB and 200GB snapshot which form part of the AMI itself. These will be stored long-term but the cost is minimal.</p>
<p><strong>FOR PART 2, SEE <a href="/project-coarse%20grained%20model/model-ctmixtures/experiment-experiment-ctmixtures/2014/09/14/ctmixtures-job-execution.html">CTMixtures Experiment Configuration and Execution</a></strong></p>
:ET